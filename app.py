from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import ChatPromptTemplate
import datetime
import streamlit as st
from iching import iching


def predict(birthday, today):
    # birthday and today could be 19990526 and 20201023
    day = str(birthday) + str(today)
    dayStr = day.replace('-', '').replace('/', '')
    dayInt = int(dayStr)
    iching.ichingDate(dayInt)
    fixPred, changePred = iching.getPredict()
    # plotTransition(6, w = 15)
    guaNames = iching.ichingName(fixPred, changePred)
    fixText = iching.ichingText(fixPred)  #
    if changePred:
        changeText = iching.ichingText(changePred)  #
    else:
        changeText = None
    sepline1 = '\n                (O--__/\__--O)'
    sepline2 = '\n(-------------(O---- |__|----O)----------------)'
    sepline4 = '\n         (-------(O-/_--_\-O)-------)'
    sepline3 = '\n(-----------(O-----/-|__|-\------O)------------)'
    ben_gua = fixText + sepline1 + sepline2 + sepline3 + sepline4
    bian_gua = changeText
    return [guaNames, ben_gua, bian_gua]


def main():
    st.title("Cyber Future Teller ğŸ§™â€â™€ï¸")
    # description
    st.write("This is a cyber fortune-teller powered by the gpt-3.5-turbo-16k model. Using the cosmic vibes of the "
             "day, it's here to unravel the mysteries of your life. Brace yourself for some AI-powered wisdom and a "
             "touch of digital divination!")

    API_KEY = st.text_input("API Key")
    d = st.date_input("When's your birthday", value=None)
    user_question = st.text_input("Please enter your question:")

    if st.button("Answer") and user_question and d and API_KEY:
        with st.spinner("Processing..."):
            # init model
            llm = ChatOpenAI(openai_api_key=API_KEY, temperature=0.9, model_name="gpt-3.5-turbo-16k")
            # iching get today's gua
            today = int(datetime.datetime.now().strftime("%Y%m%d"))
            birthday = int(d.strftime("%Y%m%d"))
            predictions = predict(birthday, today)

            # pass gua into model
            template = f"""ä½œä¸ºä¸€ä¸ªæ˜“ç»è§£å¦å¤§å¸ˆï¼Œä½ å°†åŸºäºæ¨æ¼”èµ·å¦çš„ç»“æœå›ç­”å®¢äººçš„é—®é¢˜ã€‚
                                æ¨æ¼”èµ·å¦çš„ç»“æœåŒ…æ‹¬å¦åã€æœ¬å¦å’Œå˜å¦ï¼Œç›®å‰çš„å¦ç›¸ä¸ºï¼š
                                å¦å - {predictions[0]}
                                æœ¬å¦ - {predictions[1]}
                                å˜å¦ - {predictions[2]}
                                å®¢äººå°†ä¼šè¾“å…¥ä»–çš„é—®é¢˜ã€‚
                           """
            chat_prompt = ChatPromptTemplate.from_messages([
                ("system", template),
                ("human", user_question),
            ])
            chain = chat_prompt | llm
            result = chain.invoke({"text": "colors"})
            st.write(result.content)


if __name__ == "__main__":
    main()
